\documentclass[pdftex,12pt,a4paper]{article}

\input{/home/boris/science/tex_general/title_bor_utf8}

% чисто эконометрические сокращения:
\def \hb{\hat{\beta}}
\def \hy{\hat{y}}
\def \hY{\hat{Y}}
\def \he{\hat{\varepsilon}}
\def \hCorr{\widehat{\Corr}}
\def \hVar{\widehat{\Var}}
\def \hCov{\widehat{\Cov}}


\begin{document}
\parindent=0 pt % отступ равен 0

\listoftodos

\section{Конвенция}


$y$ --- вектор столбец зависимых переменных, наблюдаемый случайный

$\beta$ --- вектор столбец неизвестных параметров, ненаблюдаемый, неслучайный

$\hy$ --- прогноз $y$ полученный по некоторой модели, наблюдаемый, случайный

$\hb$ --- оценки $\beta$

$X$ --- матрица всех объясняющих переменных

$\e$

$\he$


Некоторые авторы используют обозначения:

$Y$ и $y$ для разных вещей, $y=Y-\bar{Y}$.




\section{Семинар 1}

Неформальное определение. Если матрица $A$ квадратная, то её определителем называется площадь/объём параллелограмма/параллелепипеда образованного векторами-столбцами матрицы. Знак определителя задаётся порядком следования векторов. 


Свойства определителя:
\begin{enumerate}
\item $\det(AB)=\det(A)\det(B)=\det(BA)$, если $A$ и $B$ квадратные
\item $\det(A)=\prod \lambda_i$
\end{enumerate}


Определение. Если матрица $A$ квадратная, то её следом называется сумма диагональных элементов, $\trace(A)=\sum a_{ii}$.


Свойства следа:
\begin{enumerate}
\item $\trace(AB)=\trace(BA)$, если $AB$ и $BA$ существуют. При этом $A$ и $B$ могут не быть квадратными матрицами.
\item $\trace(A)=\sum \lambda_i$
\end{enumerate}


Добавить про геометрический смысл следа, \url{http://mathoverflow.net/questions/13526/geometric-interpretation-of-trace}.


Определение. Вектор $x$ называется собственным вектором матрицы $A$, если при умножении на матрицу $A$ он остается на той же прямой, т.е. $Ax=\lambda x$


Определение. Число $\lambda$ называется собственным числом матрицы $A$, если есть вектор $x$, который при умножении на матрицу $A$ изменяется в $\lambda$ раз, т.е. $Ax=\lambda x$.




Метод наименьших квадратов (МНК), ordinary least squares (OLS):


Есть $n$ наблюдений, $y_1$, ..., $y_n$. Есть модель, которая даёт прогнозы, $\hat{y}_1$, ..., $\hat{y}_n$. Эта модель зависит от вектора неизвестных параметров, $\beta$. МНК предлагает в качестве оценок неизвестных параметров взять такое $\hb$, чтобы минимизировать $\sum (y_i-\hat{y}_i)^2$.

\section{Семинар 2}

Контрольная-1

\section{Картинка}


Утверждение. $\sCorr^2(y,\hy)=R^2$

Доказательство. По определению, $\sCorr(y,\hy)=\frac{(y-\bar{y})(\hy-\bar{\hy})}{|y-\bar{y}||\hy-\bar{\hy}|}$. Поскольку в регрессии присутствует свободный член, $\bar{\hy}=\bar{y}$. Значит, 
\begin{equation}
\sCorr(y,\hy)=\frac{(y-\bar{y})(\hy-\bar{y})}{|y-\bar{y}||\hy-\bar{y}|}=\cos(y-\bar{y},\hy-\bar{y})
\end{equation}
По определению, $R^2=\frac{|\hy-\bar{y}|^2}{|y-\bar{y}|^2}=\cos^2(y-\bar{y},\hy-\bar{y})$



Опыт: лучший результат у меня получается с обозначением $(\bar{y},\ldots,\bar{y})$.


\section{Разное}
\begin{enumerate}

\item Гипотеза $H_0$ по-английски читается как <<H naught>>
\item При проверке гипотезы об адекватности регрессии НЕЛЬЗЯ писать $H_0: R^2=0$. 


Гипотезы имеет смысл проверять о ненаблюдаемых неизвестных константах. 
Проверить гипотезу о том, что $R^2=0$ легко. 
Для этого не нужно знать ничего из теории вероятностей, достаточно просто сравнить посчитанное значение $R^2$ с нулём. 

Более того, даже корректировка $\E(R^2)=0$ неверна. 
Случайная величина $R^2$ всегда неотрицательна, поэтому при любых разумных предпосылках на $\varepsilon$ окажется, что $\P(R^2>0)>0$. 
А это приведёт к тому, что $\E(R^2)>0$ даже если $Y$ никак не зависит от $X$.


Единственный правильный вариант --- $H_0: \beta_2=\beta_3=\ldots=\beta_k=0$ и $H_a: \exists i\geq 2 : \beta_i\neq 0$.
\end{enumerate}



\section{Ridge/Lasso regression}

LASSO --- Least Absolute Shrinkage and Selection Operator. Метод построения регрессии, предложенный Robert Tibshirani в 1995 году.

Вспомним обычный МНК:
\begin{equation}
\min_{\beta} (y-X\beta)'(y-X\beta)
\end{equation}


LASSO вместо исходной задачи решает задачу условного экстремума:
\begin{equation}
\min_{\beta} (y-X\beta)'(y-X\beta)
\end{equation}
при ограничении $\sum_{j=1}^{k}|\beta_j|\leq c$.

\todo[inline]{Проверить! Нет ли у $\beta_1$ особого положения?}

Естественно, при больших значениях $c$ результат LASSO совпадает с МНК. Что происходит при малых $c$?


Для наглядности рассмотрим задачу с двумя коэффициентами $\beta$: $\beta_1$ и $\beta_2$. Линии уровня целевой функции --- эллипсы. Допустимое множество имеет форму ромба с центром в начала координат.


\todo[inline]{на картинке три $c$: очень большое --- дающиее мнк решение, меньше --- ненулевые $\beta$, маленькое --- одна из $\beta$  равна 0}


То есть при малых $c$ LASSO обратит ровно в ноль некоторые коэффициенты $\beta$.


Применим метод множителей Лагранжа для случая, когда ограничение $\sum_{j=1}^{k}|\beta_j|\leq c$ активно, то есть выполнено как равенство. 

\begin{equation}
L(\beta,\lambda)=(y-X\beta)'(y-X\beta)+\lambda \left(\sum_{j=1}^{k}|\beta_j| - c \right)
\end{equation}

Необходимым условием первого порядка является $\partial L/\partial \beta =0$. 
Это условие первого порядка не изменится, если мы зачеркнём $c$ в выражении. 
Таким образом мы получили альтернативную формулировку метода LASSO:
\begin{equation}
\min_{\beta} (y-X\beta)'(y-X\beta)+\lambda \sum_{j=1}^{k}|\beta_j|
\end{equation}

LASSO пытается минимизировать взвешенную сумму $RSS=(y-X\beta)'(y-X\beta)$ и <<размера>> коэффициентов $\sum_{j=1}^{k}|\beta_j|$.


Мы не будем вдаваться в численные алгоритмы, которые используются при решении этой задачи.


Ridge regression отличается от LASSO ограничением $\sum \beta_j^2\leq c$. 
Также как и LASSO Ridge regression допускает альтернативную формулировку:

\begin{equation}
\min_{\beta} (y-X\beta)'(y-X\beta)+\lambda \sum_{j=1}^{k} \beta_j^2
\end{equation}

Также как и LASSO Ridge regression тоже приближает значения коэффициентов $\beta_j$ к нулю. 
Принципиальное отличие LASSO и RR. 
В LASSO краевое решение с несколькими коэффициентами равными нулю является типичной ситуацией. 
В RR коэффициент $\beta_j$ может оказаться точно равным нулю только по чистой случайности. 


LASSO допускает байесовскую интерпретацию...

Предположим, что априорное распределение параметров следующее:

...


Тогда мода апостериорного распределения будут приходится в точности (?) на оценки LASSO.


\todo[inline]{Может ли появиться мультимодальность? В точности ли на моду или только примерно?}



\end{document}