\documentclass[pdftex,12pt,a4paper]{article}

\input{/home/boris/science/tex_general/title_bor_utf8}


\begin{document}
\parindent=0 pt % отступ равен 0

\listoftodos


\section{Разное}
\begin{enumerate}
\item Гипотеза $H_0$ по-английски читается как <<H naught>>
\item При проверке гипотезы об адекватности регрессии НЕЛЬЗЯ писать $H_0: R^2=0$. 


Гипотезы имеет смысл проверять о ненаблюдаемых неизвестных константах. 
Проверить гипотезу о том, что $R^2=0$ легко. 
Для этого не нужно знать ничего из теории вероятностей, достаточно просто сравнить посчитанное значение $R^2$ с нулём. 

Более того, даже корректировка $\E(R^2)=0$ неверна. 
Случайная величина $R^2$ всегда неотрицательна, поэтому при любых разумных предпосылках на $\varepsilon$ окажется, что $\P(R^2>0)>0$. 
А это приведёт к тому, что $\E(R^2)>0$ даже если $Y$ никак не зависит от $X$.


Единственный правильный вариант --- $H_0: \beta_2=\beta_3=\ldots=\beta_k=0$ и $H_a: \exists i\geq 2 : \beta_i\neq 0$.
\end{enumerate}



\section{Ridge/Lasso regression}

LASSO --- Least Absolute Shrinkage and Selection Operator. Метод построения регрессии, предложенный Robert Tibshirani в 1995 году.

Вспомним обычный МНК:
\begin{equation}
\min_{\beta} (y-X\beta)'(y-X\beta)
\end{equation}


LASSO вместо исходной задачи решает задачу условного экстремума:
\begin{equation}
\min_{\beta} (y-X\beta)'(y-X\beta)
\end{equation}
при ограничении $\sum_{j=1}^{k}|\beta_j|\leq c$.

\todo[inline]{Проверить! Нет ли у $\beta_1$ особого положения?}

Естественно, при больших значениях $c$ результат LASSO совпадает с МНК. Что происходит при малых $c$?


Для наглядности рассмотрим задачу с двумя коэффициентами $\beta$: $\beta_1$ и $\beta_2$. Линии уровня целевой функции --- эллипсы. Допустимое множество имеет форму ромба с центром в начала координат.


\todo[inline]{на картинке три $c$: очень большое --- дающиее мнк решение, меньше --- ненулевые $\beta$, маленькое --- одна из $\beta$  равна 0}


То есть при малых $c$ LASSO обратит ровно в ноль некоторые коэффициенты $\beta$.


Применим метод множителей Лагранжа для случая, когда ограничение $\sum_{j=1}^{k}|\beta_j|\leq c$ активно, то есть выполнено как равенство. 

\begin{equation}
L(\beta,\lambda)=(y-X\beta)'(y-X\beta)+\lambda \left(\sum_{j=1}^{k}|\beta_j| - c \right)
\end{equation}

Необходимым условием первого порядка является $\partial L/\partial \beta =0$. 
Это условие первого порядка не изменится, если мы зачеркнём $c$ в выражении. 
Таким образом мы получили альтернативную формулировку метода LASSO:
\begin{equation}
\min_{\beta} (y-X\beta)'(y-X\beta)+\lambda \sum_{j=1}^{k}|\beta_j|
\end{equation}

LASSO пытается минимизировать взвешенную сумму $RSS=(y-X\beta)'(y-X\beta)$ и <<размера>> коэффициентов $\sum_{j=1}^{k}|\beta_j|$.


Мы не будем вдаваться в численные алгоритмы, которые используются при решении этой задачи.


Ridge regression отличается от LASSO ограничением $\sum \beta_j^2\leq c$. 
Также как и LASSO Ridge regression допускает альтернативную формулировку:

\begin{equation}
\min_{\beta} (y-X\beta)'(y-X\beta)+\lambda \sum_{j=1}^{k} \beta_j^2
\end{equation}

Также как и LASSO Ridge regression тоже приближает значения коэффициентов $\beta_j$ к нулю. 
Принципиальное отличие LASSO и RR. 
В LASSO краевое решение с несколькими коэффициентами равными нулю является типичной ситуацией. 
В RR коэффициент $\beta_j$ может оказаться точно равным нулю только по чистой случайности. 


LASSO допускает байесовскую интерпретацию...

Предположим, что априорное распределение параметров следующее:

...


Тогда мода апостериорного распределения будут приходится в точности (?) на оценки LASSO.


\todo[inline]{Может ли появиться мультимодальность? В точности ли на моду или только примерно?}



\end{document}