План семинара:
------------

Байесовский подход.

### Кратко:
Есть неизвестный параметр $\beta_{true}$.
Его мы не знаем, никогда не узнаем, но хотим узнать.

1. До получения данных у меня есть некоторое мнение о $\beta_{true}$
Это мнение нужно оформить в виде закона распределения величины $\beta$.
Это распределение --- априорное распределение $\beta$.

2. Есть модель, описывающая закон распределения $x_i$ при заданном $\beta$.

3. Наблюдая $x_i$ мы можем рассчитать условное распределение $\beta$ при данных $x_i$. 
Это распределение называется апостериорным распределением $\beta$. И содержит полностью 
моё мнение о $\beta_{true}$. 

Байесовский подход также допускает другую интерпретацию:
Настоящее $\beta$ случайно.

### Простейший пример. 

1. Априорно верим, что $\mu\sim N(m,h^2)$.
2. Модель для данных $x_i\sim N(\mu,\sigma^2)$. Временно считаем, что $\sigma^2$ известно.
3. Находим апостериорное распределение руками.

\mu|\bar{y}

Среднее:
\frac{\frac{1}{h^2}}{\frac{1}{h^2}+\frac{n}{\sigma^2}}m+\frac{\frac{n}{\sigma^2}}{\frac{1}{h^2}+\frac{n}{\sigma^2}}\bar{y}

Дисперсия:
\frac{1}{\frac{1}{h^2}+\frac{n}{\sigma^2}}


### Проблема:
Вывести апостериорное распределение явно можно очень редко.


### Метод Винни-Пуха (aka Gibbs sampler)
Есть 10 деревьев. На них $m_1$, $m_2$, \ldots, $m_10$ мёда.
У ВП в голове опилки, поэтому он не помнит, сколько мёда было на предыдущих деревьях. 
От одного дерева видно только, сколько мёда на нём и на соседних.
Как ВП следует передвигаться чтобы количество посещений дерева было пропорционально количеству мёда?


### Алгоритм Gibbs'а 
...




### JAGS
JAGS --- это программа, которой на вход подается описание модели в виде направленного
графа без циклов, а на выходе получаем цепь Маркова, сходяющуюся к апостериорному 
распределению
1. JAGS достаточно слозжен и конкретный алгоритм (Gibbs, Metropolis random walk, random slice, general Metropolis-Hastings) будет выбран им исходя из модели
2. Все узлы графа грубо говоря делятся на две группы: детерминированные и стохастические
Детерминированный узел y: y<-cos(x)
Стохастический узел y: y~pnorm(x,1)
Наблюдаемые переменные могут быть только стохастическими узлами.
Есть отдельные исключения из "грубо говоря":
y~dsum(a,b,c) по сути означает, что y<-a+b+c
3. Теорема, которую эксплуатирует JAGS:
f(y|G\y)~f(y|parents(y))\Prod_{a\in\children(y)}f(a|parents(a))
4. JAGS предназначен только для MCMC симуляций. Например, в нём нельзя рисовать графики и крайне неудобно осуществлять предварительную обработку данных. Поэтому удобнее JAGS использовать из R. В R осуществляется оценка простых моделей, построение графиков, преобразование данных, а в JAGS --- только MCMC.
5. Плохие сообщения об ошибках, трудно отлаживать.

### Примеры
Среднее...

Среднее с неизвестной дисперсией...

Ordered probit...

### Отличия от классического подхода (в произвольном порядке)
1. А откуда взять точечную оценку?
В качестве точечного мнения о неизвестном параметре можно взять медиану или 
математическое ожидание условного распределения.
2. А как проверить гипотезу?
Правильный ответ: гипотезы проверять нет нужды. Решения следует принимать на основании
своего полного мнения о неизвестных параметрах, т.е. на основании апостериорного закона.
Переходный ответ: в классическом подходе вместо проверки гипотез можно строить доверительный интервал. Смысл 95% интервала: при повторении исследования интервал будет 
накрывать $\beta_{true}$ в 95% случаев. Вероятность того, что данный интервал накрыл неизвестное
$\beta_{true}$ равна либо 0, либо 1, но неизвестна. В байесовском подходе можно построить 
байесовский 95% интервал. Это наиболее короткий интервал, в который $\beta$ попадает с вероятностью 95%.
3. Сравнение моделей. В классическом подходе есть стандартная процедура тестирования вложенных моделей, что делать в байесовском случае?
Правильный ответ: любая модель является упрощением действительности, поэтому при очень большом количестве наблюдений любая ограниченная модель будет отвергаться в пользу неограниченной. При большом количестве наблюдений всегда будет значимая разница между любыми двумя средними. Вопрос лишь в том, существенно ли по величине это значимое отличие.
Если мы изначально верили в непрерывное распределение $\beta$, то, естественно, 
апостериорная вероятность того, что $\beta=\beta_0$ равна нулю.
Тем не менее байесовский подход позволяет явно сравнивать модели и находить вероятность
того, что модель верна. 
Мы можем предположить, что вероятность того, что верна модель $A$ равна $p$, а вероятность
того, что верна модель $B$ - (1-p). Априорно, $p\sim U[0;1]$ и дальше оценить апостериорную
вероятность $p$. Никаких проблем с оцениванием вероятности для невложенных моделей не возникает.
4. Число наблюдений меньше числа оцениваемых параметров? Запросто! Условное распределение есть всегда.
5. Презентация результатов (посмотреть Крушке)

6. Метатеорема: Для любого статистического метода есть байесовская интерпретация.
Пример: AIC и BIC

### Комментарии к оценке:
1. Если по тестам цепь сошлась к апостериорному распределению, то это не значит, что всё хорошо. Возможно апостериорное распределение совпадает с априорным. Т.е. возможно, что коэффициент оценить невозможно в принципе. Во что изначально веришь, то и получишь.
2. Если по тестам цепь не сошлась к апостериорному распределению, то это не значит, что всё плохо. Нужно помнить, что у нас тысячи, если не миллионы (!) наблюдений. Допустим, тест обнаружил, что гипотеза о том, что по среднее по первой и второй тысячам наблюдений  совпадают отвергается. Но из-за большого количества наблюдений вполне может быть, что они отличаются на одну миллионную! А для меня такое отличие не существенно.
3. До сих пор сильно влияние традиции тестировать H0. Проверка, сошлась ли цепь, осуществляется с помощью традиционных методов, а не с помощью байесовских. Правда отчасти это вызвано большим количеством наблюдений. При большом количестве наблюдений априорные распределения не оказывают никакого влияния.


### Кратко про STAN



### Мои планы:
1. Таки реализовать multivariate ordered probit. Just for fun
2. Оформить некую коллекцую оцененных моделей с комментариями с удобным подходом для пользователя
3. Аналогично про R
github.com/bdemeshev/em301/wiki/R
4. Сделать доступное введение в STAN

Про R:
Econometrics in R
Computing for data analysis





