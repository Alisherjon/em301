# Семинар по методу максимального правдоподобия

Наши данные подчиняются следующему закону:
$x_i=i$, $y_i=4.2 x_i+\varepsilon_i$, $\varepsilon_i \sim N(0,9)$, $i=1..100$.

Создаём искуственно такие данные:
```{r "creation of the Earth in 4 lines of code"}
n.obs <- 100

x <- 1:n.obs
eps <- rnorm(n.obs,mean=0,sd=3)
y <- 4.2*x+eps
```

А теперь сделаем вид, что мы забыли, чему равны истинные значения $\beta$ и $\sigma^2$ и оценим модель 
$y_i=\beta x_i +\varepsilon_i$, где $\varepsilon_i\sim N(0,\sigma^2)$:

```{r "likelihood functions"}
log_lik <- function(beta,s2) {
  # это функция правдоподобия, она зависит от двух аргументов
  res <- -0.5*log(s2)*n.obs-0.5/s2*sum((y-beta*x)^2)
  return(res)
}

neg_log_lik <- function(params) {
  # это функция правдоподобия с противоположным знаком
  # она зависит от одного векторного аргумента - так нужно для минимизации
  res <- (-1)*log_lik(params[1],params[2])
  return(res)
}
```

```{r "optimization"}
opt.res <- nlm(neg_log_lik,c(3,2),hessian=T)
# с(3,2) - это стартовая точка. Лучшее её выбирать поближе к глобальному экстремуму. Если даже примерно не ясно, где находится глобальный экстремум, то попробуйте несколько разных стартовых значений, чтобы не попасться в локальный. Мы выбрали от фонаря.
```

```{r "extract results"}
opt.res$hessian # гессиан в точке минимума
opt.res$estimate # точка минимума
opt.res$minimum # минимум функции
```

```{r "compute confidence interval"}
var.hat <- solve(opt.res$hessian) # оценка ковариационной матрицы


estimates <- opt.res$estimate # вектор оценок неизвестных параметров
se <- sqrt(diag(var.hat)) # вектор ст. ошибок, корни из диагонали ковариационной матрицы


ci.left <- estimates - 1.96 * se
ci.right <- estimates + 1.96 * se

coef.table <- data.frame(estimates,se,ci.left,ci.right
                         )
rownames(coef.table) <- c("beta","sigma^2")
colnames(coef.table) <- c("Оценка","Ст. ошибка","Левая граница","Правая граница")

coef.table
```

Можно найти кучу примеров ["MLE in R" в гугле](https://www.google.ru/search?q=mle+in+R)

Замечания:
* Есть готовая функция для mle. Но своими руками сделать-то полезнее...
* Могут быть проблемы с отрицательным $\sigma^2$. С точки зрения R, $\sigma^2$ - это просто аргумент функции. Поэтому  при поиске минимума R может случайно залезет в область отрицательных $\sigma^2$. Естественно, к хорошим результатам это не приведёт. Что делать в этом случае? Решение 1: попробовать выбрать точку поближе к потенциальному экстремуму. Решение 2: перейти к другим параметрам, например, сделать замену $\sigma^2=e^d$. В таком случае $d$ может быть любым действительным числом.
* Лучше добавлять в функции комментарии и проверку на корректность вводимых значений.









