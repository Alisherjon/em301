\section{МНК с матрицами и вероятностями}

\begin{enumerate}

\item Пусть $y = X\beta + \e$ --- регрессионная модель.
\begin{enumerate}
\item Сформулируйте теорему Гаусса-Маркова
\item Верно ли, что оценка $\hat{\beta} = (X'X)^{-1}X'y$ несмещённая?
\item В условиях теоремы Гаусса-Маркова найдите ковариационную матрицу $\hat{\beta}$
\end{enumerate}

\item Пусть $y = X\beta + \e$ --- регрессионная модель и $\tilde{\beta} = ((X'X)^{-1}X'+ A)y$ --- несмещённая оценка вектора неизвестных параметров $\beta$. Верно ли, что $AX=0$?

\item Пусть $y = X\beta + \e$ --- регрессионная модель, $X = \begin{pmatrix} 1 & 0 \\ 1 & 1 \\ 1 & 1 \end{pmatrix}$, $y = \begin{pmatrix} 1 \\ 2 \\ 3 \end{pmatrix}$, $\beta = \begin{pmatrix} \beta_1 \\ \beta_2 \end{pmatrix}$, $\e = \begin{pmatrix} \e_1 \\ \e_2 \\ \e_3 \end{pmatrix}$, $\E(\e)$ = 0, $\Var(\e) = \sigma^2 I$. Найдите коэффициент корреляции $\Corr(\hat{\beta_1},\hat{\beta_2})$.
<<echo=FALSE>>=
X <- matrix(as.integer(c(1,1,1,0,1,1)), nrow=3, byrow = FALSE, dimnames = NULL)
y <- c(1,2,3)
@


\item Пусть $y = X\beta + \e$ --- регрессионная модель, где $\beta = \begin{pmatrix} \beta_1 \\ \beta_2 \\ \beta_3 \end{pmatrix}$. Пусть $Z = XD$, где $D = \begin{pmatrix} 1 & 1 & 0 \\ 0 & 1 & 1 \\ 0 & 0 & 1 \end{pmatrix}$. Рассмотрите <<новую>> регрессионную модель $y = Z\alpha + u$, где $\alpha = \begin{pmatrix} \alpha_1 \\ \alpha_2 \\ \alpha_3 \end{pmatrix}$. Определите, как выражаются <<новые>> МНК-коэффициенты через <<старые>>.
<<echo=FALSE,results='asis'>>=
alpha <- matrix(as.integer(c(1,0,0,1,1,0,0,1,1)), nrow=3, byrow = FALSE, dimnames = NULL)
@

\item Пусть $y = X\beta + \e$ --- регрессионная модель, где $\beta = \begin{pmatrix} \beta_1 \\ \beta_2 \\ \beta_3 \end{pmatrix}$. Пусть $Z = XD$, где $D = \begin{pmatrix} 1 & 1 & 1 \\ 0 & 1 & 1 \\ 0 & 0 & 1 \end{pmatrix}$. Рассмотрите <<новую>> регрессионную модель $y = Z\alpha + u$, где $\alpha = \begin{pmatrix} \alpha_1 \\ \alpha_2 \\ \alpha_3 \end{pmatrix}$. Определите, как выражаются <<новые>> МНК-коэффициенты через <<старые>>.
<<echo=FALSE,results='asis'>>=
alpha2 <- matrix(as.integer(c(1,0,0,1,1,0,1,1,1)), nrow=3, ncol=3, byrow = FALSE, dimnames = NULL)
@

\item Пусть $y = X\beta + \e$ --- регрессионная модель, где $\beta = \begin{pmatrix} \beta_1 \\ \beta_2 \\ \beta_3 \end{pmatrix}$. Пусть $Z = XD$, где $D = \begin{pmatrix} 1 & 0 & 0 \\ 0 & 1 & 1 \\ 0 & 0 & 1 \end{pmatrix}$. Рассмотрите <<новую>> регрессионную модель $y = Z\alpha + u$, где $\alpha = \begin{pmatrix} \alpha_1 \\ \alpha_2 \\ \alpha_3 \end{pmatrix}$. Определите, как выражаются <<новые>> МНК-коэффициенты через <<старые>>.
<<echo=FALSE,results='asis'>>=
alpha3 <- matrix(as.integer(c(1,0,0,0,1,0,0,1,1)), nrow=3, ncol=3, byrow = FALSE, dimnames = NULL)
@


\item Пусть $y = X\beta + \e$ --- регрессионная модель. Верно ли, что $\hat{\e}'\hat{y}=0$ и $\hat{y}'\hat{\e}=0$?
\solution{да, да}

\item Пусть $y = X\beta + \e$ --- регрессионная модель, где $\E(\e)=0$, $\Var(\e)=\sigma_{\e}^2 I$. Пусть $A$ --- неслучайная матрица размера $k \times k$, $\det(A) \not= 0.$ Совершается преобразование регрессоров по правилу $Z=XA$. В преобразованных регрессорах уравнение выглядит так: $y = Z\gamma + u$, где $\E(u)=0$, $\Var(u)=\sigma_{u}^2 I.$

\begin{enumerate}
\item Как связаны между собой МНК-оценки $\hat{\beta}$ и $\hat{\gamma}$?
\item Как связаны между собой векторы остатков регрессий?
\item Как связаны между собой прогнозные значения, полученные по двум регрессиям?
\end{enumerate}

\solution{
\begin{enumerate}
\item $\hat{\gamma} = (Z'Z)^{-1}Z'y = A^{-1}(X'X)^{-1}(A')^{-1}A'X'y = A^{-1}(X'X)^{-1} X'y = A^{-1}\hat{\beta}$
\item $\hat{u} = y - Z\hat{\gamma} = y - XAA^{-1}\hat{\beta} = y - X\hat{\beta} = \hat{\e}$
\item Пусть $z^0 = \begin{pmatrix} 1 & z_1^0 & \dots & z_{k-1}^0 \end{pmatrix}$ --- вектор размера $1 \times k$ и $x^0 = \begin{pmatrix} 1 & x_1^0 & \dots & x_{k-1}^0 \end{pmatrix}$ --- вектор размера $1 \times k$. Оба эти вектора представляют собой значения факторов. Тогда $z^0 = x^0 A$ и прогнозное значение для регрессии с преобразованными факторами равно $z^0 \hat{\gamma} = x^0 AA^{-1} \hat{\beta} = x^0 \hat{\beta}$ прогнозному значению для регрессии с исходными факторами. 
\end{enumerate}
}

\item Рассмотрим оценку вида $\tilde{\beta} = ((X'X)^{-1} + \gamma I)X'y$ для вектора коэффициентов регрессионного уравнения $y = X\beta + \e$, удовлетворяющего условиям классической регрессионной модели. Найдите $\E(\tilde{\beta})$ и $\Var(\tilde{\beta}).$ 

\solution{
\begin{enumerate}
\item $\E(\tilde{\beta}) = ((X'X)^{-1} + \gamma I)X'\E(y) = ((X'X)^{-1} + \gamma I)X'X\beta = \beta + \gamma X'X\beta$
\item $\Var(\tilde{\beta}) = \Var(((X'X)^{-1} + \gamma I)X'y) = \Var(((X'X)^{-1} + \gamma I)X'\e) = $

$ = (((X'X)^{-1} + \gamma I)X')\Var(\e)(((X'X)^{-1} + \gamma I)X')'= $

$ = (((X'X)^{-1} + \gamma I)X')\sigma_{\e}^2 I(((X'X)^{-1} + \gamma I)X')'= 
\sigma_{\e}^2((X'X)^{-1} + \gamma I)X'X((X'X)^{-1} + \gamma I) = $

$ = \sigma_{\e}^2((X'X)^{-1} + \gamma I)(I + \gamma X'X) = \sigma_{\e}^2((X'X)^{-1} + 2\gamma I + \gamma ^2 X'X)$
\end{enumerate}
}


\item Верно ли, что при невырожденном преобразовании факторов $R^2$ не меняется? А именно, пусть заданы две регрессионные модели: $y = X\beta + \e$ и $y = Z\alpha + u$, где $y$ --- вектор размера $n \times 1$, $X$ и $Z$ --- матрицы размера $n \times k$, $\beta$ и $\alpha$ --- вектора рамзера $k \times 1$, $\e$ и $u$ --- вектора размера $n \times 1$, а также $Z=XD$, $\det(D) \not= 0.$ Верно ли, что коэффициенты детерминации представленных выше моделей равны между собой?

\item Верно ли, что при невырожденном преобразовании факторов $RSS$ не меняется. А именно, пусть заданы две регрессиионные модели: $y = X\beta + \e$ и $y = Z\alpha + u$, где $y$ --- вектор размера $n \times 1$, $X$ и $Z$ --- матрицы размера $n \times k$, $\beta$ и $\alpha$ --- вектора рамзера $k \times 1$, $\e$ и $u$ --- вектора размера $n \times 1$, а также $Z=XD$, $\det(D) \not= 0.$ Верно ли, что сумма квадратов остатков в представленных выше моделях равны между собой?


\item Пусть регрессионная модель $y_i = \beta_1 + \beta_2 x_{i2} + \beta_3 x_{i3} + \e_i$, $i = 1, \ldots, n$, задана в матричном виде при помощи уравнения $y = X \beta + \e$, где $\beta =  \begin{pmatrix}
\beta_1 & \beta_2 & \beta_3\\
\end{pmatrix} ^T$. Известно, что $\E \e = 0$ и $\Var (\e) = 4 \cdot I$. Известно также, что:

$y =  \begin{pmatrix}
1 \\
2 \\
3 \\
4 \\
5 \\
\end{pmatrix} $, $X =  \begin{pmatrix}
1 & 0 & 0 \\
1 & 0 & 0 \\
1 & 1 & 0 \\
1 & 1 & 0 \\
1 & 1 & 1 \\
\end{pmatrix} $

Для удобства расчётов ниже приведены матрицы:

$X^T X =  \begin{pmatrix}
5 & 3 & 1 \\
3 & 3 & 1 \\
1 & 1 & 1 \\
\end{pmatrix} $ и $(X^T X)^{-1} =  \begin{pmatrix}
0.5 & -0.5 & 0 \\
-0.5 & 1 & -0.5 \\
0 & -0.5 & 1.5 \\
\end{pmatrix} $.

Найдите:
\begin{enumerate}
\item $\Var (\e_1)$
\item $\Var (\beta_1)$
\item $\Var (\hat{\beta}_1)$
\item $\widehat{\Var }(\hat{\beta}_1)$
\item $\E (\hat{\beta}_1^2) - \beta_1^2$
\item $\Cov (\hat{\beta}_2, \hat{\beta}_3)$
\item $\widehat{\Cov }(\hat{\beta}_2, \hat{\beta}_3)$
\item $\Var (\hat{\beta}_2 - \hat{\beta}_3)$
\item $\widehat{\Var }(\hat{\beta}_2 - \hat{\beta}_3)$
\item $\Var (\beta_2 - \beta_3)$
\item $\Corr (\hat{\beta}_2, \hat{\beta}_3)$
\item $\widehat{\Corr}(\hat{\beta}_2, \hat{\beta}_3)$
\item $\E (\hat{\sigma}^2)$
\item $\hat{\sigma}^2$
\end{enumerate}



\item Пусть $y_i = \beta_1 + \beta_2 x_{1i} + \beta_3 x_{2i} + \e_i$ --- регрессионная модель, где $X = \begin{pmatrix} 1 & 0 & 0 \\ 1 & 0 & 0 \\ 1 & 0 & 0 \\ 1 & 1 & 0 \\ 1 & 1 & 1 \end{pmatrix}$, $y = \begin{pmatrix} 1 \\ 2 \\ 3 \\ 4 \\ 5 \end{pmatrix}$, $\beta = \begin{pmatrix} \beta_1 \\ \beta_2 \\ \beta_3 \end{pmatrix}$, $\e = \begin{pmatrix} \e_1 \\ \e_2 \\ \e_3 \\ \e_4 \\ \e_5  \end{pmatrix}$, ошибки $\e_i$ независимы и нормально распределены с $\E(\e)$ = 0, $Var(\e) = \sigma^2 I$. Для удобства расчётов даны матрицы: $X'X = \begin{pmatrix} 5 & 2 & 1 \\ 2 & 2 & 1\\ 1 & 1 & 1 \end{pmatrix}$ и $(X'X)'= \begin{pmatrix} 0.3333 & -0.3333 & 0.0000 \\ -0.3333 & 1.3333 & -1.0000 \\ 0.0000 & -1.0000 & 2.0000 \end{pmatrix}$


<<echo=FALSE,results='asis'>>=
x <- matrix(as.integer(c(1,1,1,1,1,0,0,0,1,1,0,0,0,0,1)), nrow=5, ncol=3, byrow = FALSE, dimnames = NULL)
y <- matrix(as.integer(c(1,2,3,4,5)), nrow=5, ncol=1, byrow = FALSE, dimnames = NULL)
xx <- matrix(as.integer(c(5,2,1,2,2,1,1,1,1)), nrow=3, ncol=3, byrow = FALSE, dimnames = NULL)
x_x <- solve(xx)
@


\begin{enumerate}
\item Укажите число наблюдений
\item Укажите число регрессоров в модели, учитывая свободный член
\item Найдите $TSS = \sum_{i=1}^n (y_i - \bar y)^2$
\item Найдите $RSS = \sum_{i=1}^n (y_i - \hat{y_i})^2$
\item Методом МНК найдите оценку для вектора неизвестных коэффициентов
\item Чему равен $R^2$ в модели? Прокомментируйте полученное значение с точки зрения качества оценённого уравнения регрессии
\item Сформулируйте основную и альтернативную гипотезы, которые соответствуют тесту на значимость переменной $x_1$ в уравнении регрессии
\item Протестируйте на значимость переменную $x_1$ в уравнении регрессии на уровне значимости $10\%$:
\begin{enumerate}
\item Приведите формулу для тестовой статистики 
\item Укажите распределение тестовой статистики
\item Вычислите наблюдаемое значение тестовой статистики
\item Укажите границы области, где основная гипотеза не отвергается
\item Сделайте статистический вывод о значимости переменной $x_1$
\end{enumerate}
\item Найдите $P-$значение, соответствующее наблюдаемому значению тестовой статистики $(T_{obs})$ из предыдущего пункта. На основе полученного $P-$значения сделайте вывод о значимости переменной $x_1$
\item На уровне значимости $10\%$ проверьте гипотезу $H_0: \beta_1 = 1$ против альтернативной $H_a: \beta_1 \not= 1$:
\begin{enumerate}
\item Приведите формулу для тестовой статистики 
\item Укажите распределение тестовой статистики
\item Вычислите наблюдаемое значение тестовой статистики
\item Укажите границы области, где основная гипотеза не отвергается
\item Сделайте статистический вывод
\end{enumerate}
\item На уровне значимости $10\%$ проверьте гипотезу $H_0: \beta_1 = 1$ против альтернативной $H_a: \beta_1 > 1$:
\begin{enumerate}
\item Приведите формулу для тестовой статистики 
\item Укажите распределение тестовой статистики
\item Вычислите наблюдаемое значение тестовой статистики
\item Укажите границы области, где основная гипотеза не отвергается
\item Сделайте статистический вывод
\end{enumerate}
\item На уровне значимости $10\%$ проверьте гипотезу $H_0: \beta_1 = 1$ против альтернативной $H_a: \beta_1 < 1$:
\begin{enumerate}
\item Приведите формулу для тестовой статистики 
\item Укажите распределение тестовой статистики
\item Вычислите наблюдаемое значение тестовой статистики
\item Укажите границы области, где основная гипотеза не отвергается
\item Сделайте статистический вывод
\end{enumerate}
\item Сформулируйте основную гипотезу, которая соответствует тесту на значимость регрессии <<в целом>>
\item На уровне значимости $5\%$ проверьте гипотезу о значимости регрессии <<в целом>>:
\begin{enumerate}
\item Приведите формулу для тестовой статистики 
\item Укажите распределение тестовой статистики
\item Вычислите наблюдаемое значение тестовой статистики
\item Укажите границы области, где основная гипотеза не отвергается
\item Сделайте статистический вывод
\end{enumerate}
\item Найдите $P-$значение, соответствующее наблюдаемому значению тестовой статистики $(T_{obs})$ из предыдущего пункта. На основе полученного $P-$значения сделайте вывод о значимости регрессии <<в целом>>
\item На уровне значимости $5\%$ проверьте гипотезу  $H_0: \beta_1 + \beta_2 = 2$ против альтернативной $H_a: \beta_1 + \beta_2 \not= 2$:
\begin{enumerate}
\item Приведите формулу для тестовой статистики 
\item Укажите распределение тестовой статистики
\item Вычислите наблюдаемое значение тестовой статистики
\item Укажите границы области, где основная гипотеза не отвергается
\item Сделайте статистический вывод
\end{enumerate}
\item На уровне значимости $5\%$ проверьте гипотезу  $H_0: \beta_1 + \beta_2 = 2$ против альтернативной $H_a: \beta_1 + \beta_2 > 2$:
\begin{enumerate}
\item Приведите формулу для тестовой статистики 
\item Укажите распределение тестовой статистики
\item Вычислите наблюдаемое значение тестовой статистики
\item Укажите границы области, где основная гипотеза не отвергается
\item Сделайте статистический вывод
\end{enumerate}
\item На уровне значимости $5\%$ проверьте гипотезу  $H_0: \beta_1 + \beta_2 = 2$ против альтернативной $H_a: \beta_1 + \beta_2 < 2$:
\begin{enumerate}
\item Приведите формулу для тестовой статистики 
\item Укажите распределение тестовой статистики
\item Вычислите наблюдаемое значение тестовой статистики
\item Укажите границы области, где основная гипотеза не отвергается
\item Сделайте статистический вывод
\end{enumerate}
\end{enumerate}

\solution{

\begin{enumerate}
\item $n = 5$
\item $k = 3$
\item $TSS = 10$
\item $RSS = 2$
\item $\hat{\beta} = \begin{pmatrix} \hat{\beta}_1 \\ \hat{\beta}_2 \\ \hat{\beta}_3 \end{pmatrix} = (X'X)^{-1}X'y = \begin{pmatrix} 2 \\ 2 \\ 1 \end{pmatrix}$
\item $R^2 = 1 - \frac {RSS}{TSS} = 0.8.$ $R^2$ высокий, построенная эконометрическая модель <<хорошо>> описывает данные
\item Основная гипотеза --- $H_0: \beta_1 = 0$, альтернативная гипотеза --- $H_a: \beta_1 \not= 0$
\item Проверка гипотезы
\begin{enumerate}
\item $T = \frac{\hat{\beta}_1 - \beta_1}{\sqrt{\widehat{Var}(\hat{\beta}_1)}} = \frac {\hat{\beta}_1 - \beta_1}{\sqrt{{\frac{RSS}{n-k}}[(X'X)^{-1}]_{22}}}; n = 5; k = 3$
\item $T \sim t(n-k); n = 5; k = 3$
\item $T_{obs} = \frac{\hat{\beta}_1 - 0}{\sqrt{\widehat{Var}(\hat{\beta}_1)}} = \frac {\hat{\beta}_1 - 0}{\sqrt{{\frac{RSS}{n-k}}[(X'X)^{-1}]_{22}}} = \frac{2-0}{\sqrt{{\frac{2}{5-3}}1.3333}} = 1.7321$
\item Нижняя граница $= -2.920$, верхняя граница $= 2.920$
\item Поскольку $T_{obs} = 1.7321$, что принадлежит промежутку от -2.920 до 2.920, то на основе имеющихся данных нельзя отвергнуть основную гипотезу на уровне значимости $10\%$
\end{enumerate}
\item $p-value(T_{obs}) = \mathbb{P}(|T|>|T_{obs}|) = 2F_{T}(|T_{obs}|)$, где $F_{T}(|T_{obs}|)$ --- функция распределения $t-$распределения с $n - k = 5 - 3 = 2$ степенями свободы в точке $|T_{obs}|$. $p-value(T_{obs}) = 2tcdf(-|T_{obs}|, n - k) = 2tcdf(-1.7321,2) = 0.2253$. Поскольку $P-$значение превосходит уровень значимости $10\%$, то основная гипотеза --- $H_0: \beta_1 = 0$ не может быть отвергнута
\item Проверка гипотезы
\begin{enumerate}
\item $T = \frac{\hat{\beta}_1 - \beta_1}{\sqrt{\widehat{Var}(\hat{\beta}_1)}} = \frac {\hat{\beta}_1 - \beta_1}{\sqrt{{\frac{RSS}{n-k}}[(X'X)^{-1}]_{22}}}; n = 5; k = 3$
\item $T \sim t(n-k); n = 5; k = 3$
\item $T_{obs} = \frac{\hat{\beta}_1 - 1}{\sqrt{\widehat{Var}(\hat{\beta}_1)}} = \frac {\hat{\beta}_1 - 1}{\sqrt{{\frac{RSS}{n-k}}[(X'X)^{-1}]_{22}}} = \frac{2-1}{\sqrt{{\frac{2}{5-3}}1.3333}} = 0.8660$
\item Нижняя граница $= -2.920$, верхняя граница $= 2.920$
\item Поскольку $T_{obs} = 0.8660$, что принадлежит промежутку от -2.920 до 2.920, то на основе имеющихся данных нельзя отвергнуть основную гипотезу на уровне значимости $10\%$
\end{enumerate}
\item Проверка гипотезы
\begin{enumerate}
\item $T = \frac{\hat{\beta}_1 - \beta_1}{\sqrt{\widehat{Var}(\hat{\beta}_1)}} = \frac {\hat{\beta}_1 - \beta_1}{\sqrt{{\frac{RSS}{n-k}}[(X'X)^{-1}]_{22}}}; n = 5; k = 3$
\item $T \sim t(n-k); n = 5; k = 3$
\item $T_{obs} = \frac{\hat{\beta}_1 - 1}{\sqrt{\widehat{Var}(\hat{\beta}_1)}} = \frac {\hat{\beta}_1 - 1}{\sqrt{{\frac{RSS}{n-k}}[(X'X)^{-1}]_{22}}} = \frac{2-1}{\sqrt{{\frac{2}{5-3}}1.3333}} = 0.8660$
\item Нижняя граница $= -\infty$, верхняя граница $= 1.8856$
\item Поскольку $T_{obs} = 0.8660$, что принадлежит промежутку от $-\infty$ до $1.8856$, то на основе имеющихся данных нельзя отвергнуть основную гипотезу на уровне значимости $10\%$
\end{enumerate}
\item Проверка гипотезы
\begin{enumerate}
\item $T = \frac{\hat{\beta}_1 - \beta_1}{\sqrt{\widehat{Var}(\hat{\beta}_1)}} = \frac {\hat{\beta}_1 - \beta_1}{\sqrt{{\frac{RSS}{n-k}}[(X'X)^{-1}]_{22}}}; n = 5; k = 3$
\item $T \sim t(n-k); n = 5; k = 3$
\item $T_{obs} = \frac{\hat{\beta}_1 - 1}{\sqrt{\widehat{Var}(\hat{\beta}_1)}} = \frac {\hat{\beta}_1 - 1}{\sqrt{{\frac{RSS}{n-k}}[(X'X)^{-1}]_{22}}} = \frac{2-1}{\sqrt{{\frac{2}{5-3}}1.3333}} = 0.8660$
\item Нижняя граница $= -1.8856$, верхняя граница $= +\infty$
\item Поскольку $T_{obs} = 0.8660$, что принадлежит промежутку от $-1.8856$ до $+\infty$, то на основе имеющихся данных нельзя отвергнуть основную гипотезу на уровне значимости $10\%$
\end{enumerate}
\item Основная гипотеза --- $H_0: \beta_1 = \beta_2 = 0$, альтернативная гипотеза --- $H_a: |\beta_1| + |\beta_2| > 0$
\item Проверка гипотезы
\begin{enumerate}
\item $T = \frac{R^2}{1 - R^2} \cdot \frac{n-k}{k}; n = 5; k = 3$
\item $T \sim F(n-k); n = 5; k = 3$
\item $T_{obs} = \frac{R^2}{1 - R^2} \cdot \frac{n-k}{k} = \frac{0.8}{1 - 0.8} \cdot \frac{5-3}{2} = 4$
\item Нижняя граница $= 0$, верхняя граница $= 19$
\item Поскольку $T_{obs} = 4$, что принадлежит промежутку от $0$ до $19$, то на основе имеющихся данных нельзя отвергнуть основную гипотезу на уровне значимости $5\%$. Следовательно, регрессия в целом незначима. Напомним, что $R^2 = 0.8$, то есть он высокий. Но при этом регрессия <<в целом>> незначима. Такой эффект может возникать при малом объёме выборки, например, таком, как в данной задаче
\end{enumerate}
\item $p-value(T_{obs}) = \mathbb{P}(|T|>|T_{obs}|) = 2F_{T}(|T_{obs}|)$, где $F_{T}(|T_{obs}|)$ --- функция распределения $F-$распределения c $k = 3$ и $n - k = 5 - 3 = 2$ степенями свободы в точке $T_{obs}$. $p-value(T_{obs}) = 1 - fcdf(-|T_{obs}|, n - k) = 1 - fcdf(4,2) = 0.2$. Поскольку $P-$значение превосходит уровень значимости $10\%$, то основная гипотеза --- $H_0: \beta_1 = \beta_2 = 0$ не может быть отвергнута. Таким образом, регрессия <<в целом>> незначима
\item Проверка гипотезы
\begin{enumerate}
\item $T = \frac{\hat{\beta}_1 + \hat{\beta}_2 - (\beta_1 + \beta_2)}{\sqrt{\widehat{\Var}(\hat{\beta}_1 + \hat{\beta}_2)}}$, где $\widehat{\Var}(\hat{\beta}_1 + \hat{\beta}_2) = \widehat{\Var}(\hat{\beta}_1) + \widehat{\Var}(\hat{\beta}_2) + 2\widehat{\Cov}(\hat{\beta}_1;\hat{\beta}_2) = \hat{\sigma}^2 [(X'X)^{-1}]_{22} + 2\hat{\sigma}^2 [(X'X)^{-1}]_{23} + \hat{\sigma}^2 [(X'X)^{-1}]_{33}= \frac{RSS}{n - k}([(X'X)^{-1}]_{22} + 2[(X'X)^{-1}]_{23} + [(X'X)^{-1}]_{33})$
\item $T \sim t(n-k); n = 5; k = 3$
\item $\widehat{\Var}(\hat{\beta}_1 + \hat{\beta}_2) = \frac{RSS}{n - k}([(X'X)^{-1}]_{22} + 2[(X'X)^{-1}]_{23} + [(X'X)^{-1}]_{33}) = \frac{2}{5 - 3} (1.3333 + 2(-1.0000) + 2.0000) = 1.3333.$ Тогда $T_{obs} = \frac{\hat{\beta}_1 + \hat{\beta}_2 - 2}{\sqrt{\widehat{\Var}(\hat{\beta}_1 + \hat{\beta}_2)}} = \frac{2 + 1 - 2}{\sqrt{1.3333}} = 0.8660$
\item Нижняя граница $= - 4.3027$, верхняя граница $= 4.3027$
\item Поскольку $T_{obs} = 0.8660$, что принадлежит промежутку от $- 4.3027$ до $4.3027$, то на основе имеющихся данных нельзя отвергнуть основную гипотезу на уровне значимости $5\%$
\end{enumerate}
\item Проверка гипотезы
\begin{enumerate}
\item $T = \frac{\hat{\beta}_1 + \hat{\beta}_2 - (\beta_1 + \beta_2)}{\sqrt{\widehat{\Var}(\hat{\beta}_1 + \hat{\beta}_2)}}$, где $\widehat{\Var}(\hat{\beta}_1 + \hat{\beta}_2) = \widehat{\Var}(\hat{\beta}_1) + \widehat{\Var}(\hat{\beta}_2) + 2\widehat{\Cov}(\hat{\beta}_1;\hat{\beta}_2) = \hat{\sigma}^2 [(X'X)^{-1}]_{22} + 2\hat{\sigma}^2 [(X'X)^{-1}]_{23} + \hat{\sigma}^2 [(X'X)^{-1}]_{33}= \frac{RSS}{n - k}([(X'X)^{-1}]_{22} + 2[(X'X)^{-1}]_{23} + [(X'X)^{-1}]_{33})$
\item $T \sim t(n-k); n = 5; k = 3$
\item $\widehat{\Var}(\hat{\beta}_1 + \hat{\beta}_2) = \frac{RSS}{n - k}([(X'X)^{-1}]_{22} + 2[(X'X)^{-1}]_{23} + [(X'X)^{-1}]_{33}) = \frac{2}{5 - 3} (1.3333 + 2(-1.0000) + 2.0000) = 1.3333.$ Тогда $T_{obs} = \frac{\hat{\beta}_1 + \hat{\beta}_2 - 2}{\sqrt{\widehat{\Var}(\hat{\beta}_1 + \hat{\beta}_2)}} = \frac{2 + 1 - 2}{\sqrt{1.3333}} = 0.8660$
\item Нижняя граница $= - \infty$, верхняя граница $= 2.9200$
\item Поскольку $T_{obs} = 0.8660$, что принадлежит промежутку от $- \infty$ до $2.9200$, то на основе имеющихся данных нельзя отвергнуть основную гипотезу на уровне значимости $5\%$
\end{enumerate}
\item Проверка гипотезы
\begin{enumerate}
\item $T = \frac{\hat{\beta}_1 + \hat{\beta}_2 - (\beta_1 + \beta_2)}{\sqrt{\widehat{\Var}(\hat{\beta}_1 + \hat{\beta}_2)}}$, где $\widehat{\Var}(\hat{\beta}_1 + \hat{\beta}_2) = \widehat{\Var}(\hat{\beta}_1) + \widehat{\Var}(\hat{\beta}_2) + 2\widehat{\Cov}(\hat{\beta}_1;\hat{\beta}_2) = \hat{\sigma}^2 [(X'X)^{-1}]_{22} + 2\hat{\sigma}^2 [(X'X)^{-1}]_{23} + \hat{\sigma}^2 [(X'X)^{-1}]_{33}= \frac{RSS}{n - k}([(X'X)^{-1}]_{22} + 2[(X'X)^{-1}]_{23} + [(X'X)^{-1}]_{33})$
\item $T \sim t(n-k); n = 5; k = 3$
\item $\widehat{\Var}(\hat{\beta}_1 + \hat{\beta}_2) = \frac{RSS}{n - k}([(X'X)^{-1}]_{22} + 2[(X'X)^{-1}]_{23} + [(X'X)^{-1}]_{33}) = \frac{2}{5 - 3} (1.3333 + 2(-1.0000) + 2.0000) = 1.3333.$ Тогда $T_{obs} = \frac{\hat{\beta}_1 + \hat{\beta}_2 - 2}{\sqrt{\widehat{\Var}(\hat{\beta}_1 + \hat{\beta}_2)}} = \frac{2 + 1 - 2}{\sqrt{1.3333}} = 0.8660$
\item Нижняя граница $= - 2.9200$, верхняя граница $= + \infty$
\item Поскольку $T_{obs} = 0.8660$, что принадлежит промежутку от $-  2.9200$ до $+ \infty$, то на основе имеющихся данных нельзя отвергнуть основную гипотезу на уровне значимости $5\%$
\end{enumerate}
\end{enumerate}
} % end of \solution{ 




\item Пусть $y = X\beta + \e$ --- регрессионная модель, где $X = \begin{pmatrix} 1 & 0 & 0 \\ 1 & 0 & 0 \\ 1 & 1 & 0 \\ 1 & 1 & 0 \\ 1 & 1 & 1 \end{pmatrix}$, $y = \begin{pmatrix} 1 \\ 2 \\ 3 \\ 4 \\ 5 \end{pmatrix}$, $\beta = \begin{pmatrix} \beta_1 \\ \beta_2 \\ \beta_3 \end{pmatrix}$, $\e = \begin{pmatrix} \e_1 \\ \e_2 \\ \e_3 \\ \e_4 \\ \e_5  \end{pmatrix}$, $\E(\e)$ = 0, $Var(\e) = \sigma^2 I$.

<<echo=FALSE,results='asis'>>=
x <- matrix(as.integer(c(1,1,1,1,1,0,0,1,1,1,0,0,0,0,1)), nrow=5, ncol=3, byrow = FALSE, dimnames = NULL)
y <- matrix(as.integer(c(1,2,3,4,5)), nrow=5, ncol=1, byrow = FALSE, dimnames = NULL)
@

На уровне значимости $5\%$ проверьте гипотезу  $H_0: \beta_1 + \beta_2 = 2$ против альтернативной $H_a: \beta_1 + \beta_2 \not= 2$:
\begin{enumerate}
\item Приведите формулу для тестовой статистики 
\item Укажите распределение тестовой статистики
\item Вычислите наблюдаемое значение тестовой статистики
\item Укажите границы области, где основная гипотеза не отвергается
\item Сделайте статистический вывод
\end{enumerate}



\item По 13 наблюдениям Вася оценил модель со свободным членом, пятью количественными регрессорами и двумя качественными. Качественные регрессоры Вася правильно закодировал с помощью дамми-переменных. Одна качественная переменная принимала четыре значения, другая --- пять.

\begin{enumerate}
\item Найдите $SSR$, $R^2$
\item Как выглядит матрица $X(X'X)^{-1}X'$?
\item Почему 13 --- несчастливое число?
\end{enumerate} 


\item В рамках классической линейной модели найдите все математические ожидания и все ковариационные матрицы всех пар случайных векторов: $\ve$, $y$, $\hy$, $\he$, $\hb$. Т.е. найдите $\E(\e)$, $\E(y)$, \ldots и $\Cov(\e,y)$, $\Cov(\e,\hy)$, \ldots
\solution{$\Var(\hb)=\sigma^2 (X'X)^{-1}$}

\item Найдите $\E(\sum (\e_i-\bar{\e})^2 )$, $\E(RSS)$
\solution{ $(n-1)\sigma^2$, $(n-k)\sigma^2$}

\item Используя матрицы $P=X(X'X)^{-1}X'$ и $\pi=\v1(\v1'\v1)^{-1}\v1'$ запишите $RSS$, $TSS$ и $ESS$ в матричной форме
\solution{$TSS=y'(I-\pi)y$, $RSS=y'(I-P)y$, $ESS=y'(P-\pi)y$ }


\item $\E(TSS)$, $\E(ESS)$ --- громоздкие 
\solution{ $\E(TSS)=(n-1)\sigma^2+\beta'X'(I-\pi)X\beta$}

\item Вася строит регрессию $y$ на некий набор объясняющих переменных и константу. А на самом деле $y_i=\beta_1+\e_i$. Чему равно $\E(TSS)$, $\E(RSS)$, $\E(ESS)$ в этом случае?
\solution{ $(n-1)\sigma^2$, $(n-k)\sigma^2$, $(k-1)\sigma^2$}


\item Рассмотрим классическую линейную модель. Являются ли векторы $\he$ и $\hy$ перпендикулярными? Найдите $\Cov(\he,\hy)$

\item Чему в классической модели регрессии равны $\E(\e)$, $\E(\he)$? Верно ли что $\sum \e_i$ равна 0? Верно ли что $\sum \he_i$ равна 0?


\item Найдите на Картинке все перпендикулярные векторы. Найдите на Картинке все прямоугольные треугольники. Сформулируйте для них теоремы Пифагора.
\solution{$\sum y_i^2=\sum \hy_i^2+\sum \he_i^2$, $TSS=ESS+RSS$, }

\item Покажите на Картинке TSS, ESS, RSS, $R^2$, $\sCov(\hy,y)$
\solution{}


\item Предложите аналог $R^2$ для случая, когда константа среди регрессоров отсутствует. Аналог должен быть всегда в диапазоне $[0;1]$, совпадать с обычным $R^2$, когда среди регрессоров есть константа, равняться единице в случае нулевого $\he$.
\solution{Спроецируем единичный столбец на <<плоскость>>, обозначим его $1'$. Делаем проекцию $y$ на <<плоскость>> и на $1'$. Далее аналогично. }

\item Вася оценил регрессию $y$ на константу, $x$ и $z$. А затем, делать ему нечего, регрессию $y$ на константу и полученный $\hy$. Какие оценки коэффициентов у него получатся? Чему будет равна оценка дисперсии коэффицента при $\hy$? Почему оценка коэффициента неслучайна, а оценка её дисперсии положительна?
\solution{Проекция $y$ на $\hy$ это $\hy$, поэтому оценки коэффициентов будут 0 и 1. Оценка дисперсии $\frac{RSS}{(n-2)ESS}$. Нарушены предпосылки теоремы Гаусса-Маркова, например, ошибки новой модели в сумме дают 0, значит коррелированы. } 


\item При каких условиях $TSS=ESS+RSS$?
\solution{Либо в регрессию включена константа, либо единичный столбец (тут была опечатка, столбей) можно получить как линейную комбинацию регрессоров, например, включены дамми-переменные для каждого возможного значения качественной переменной.}


\item Истинная модель имеет вид $y=X\beta +\e$. Вася оценивает модель $\hy=X \hb$ по первой части выборки, получает $\hb_a$, по второй части выборки --- получает $\hb_b$ и по всей выборке --- $\hb_{tot}$. Как связаны между собой $\hb_a$, $\hb_b$, $\hb_{tot}$? Как связаны между собой ковариационные матрицы $\Var(\hb_a)$,  $\Var(\hb_b)$ и  $\Var(\hb_{tot})$?
\solution{Сами оценки коэффициентов никак детерминистически не связаны, но при большом размере подвыборок примерно равны. А ковариационные матрицы связаны соотношением $\Var(\hb_a)^{-1}+\Var(\hb_b)^{-1}=\Var(\hb_{tot})^{-1}$ }



\item Модель линейной регрессии имеет вид $y_i=\b_1 x_{i,1}+\b_2 x_{i,2} + u_i$.
Сумма квадратов остатков имеет вид $Q\left(\hb_1,\hb_2\right)=\sum_{i=1}^n (y_1-\hb_1 x_{i,1}-\hb_2 x_{i,2})^2$.
\begin{enumerate}
\item Выпишите необходимые условия минимума суммы квадратов остатков
\item Найдите матрицу $X'X$ и вектор $X'y$ если матрица $X$ имеет вид
$X=
\left(
\begin{array}{cc}
x_{1,1} & x_{1,2} \\
\vdots & \vdots \\
x_{n,1} & x_{n,2}
\end{array}
\right)
$,
а вектор $y$ имеет вид
$y=
\left(
\begin{array}{c}
y_1 \\
\vdots \\
y_n
\end{array}
\right)
$
\item Докажите, что необходимые условия равносильны матричному уравнению $X'X\hb=X'y$, где
$\hb=
\left(
\begin{array}{c}
\hb_1 \\
\hb_2
\end{array}
\right)
$
\item Предполагая, что матрица $X'X$ обратима, найдите $\hb$
\end{enumerate}



\item Вася оценил исходную модель:
\[
y_i=\b_1+\b_2 x_i + u_i
\]

Для надежности Вася стандартизировал переменные, т.е. перешёл к $y_i^*=(y_i-\bar{y})/s_y$ и $x_i^*=(x_i-\bar{x})/s_x$. Затем Вася оценил ещё две модели:

\[
y_i^*=\b_1'+\b_2' x^*_i + u_i'
\]

и 
\[
y_i^*=\b_2'' x^*_i + u_i''
\]

В решении можно считать $s_x$ и $s_y$ известными.
 
\begin{enumerate}
\item Найдите $\hb_1'$
\item Как связаны между собой $\hb_2$, $\hb_2'$ и $\hb_2''$? 
\item Как связаны между собой $\hat{u}_i$, $\hat{u}_i'$ и $\hat{u}_i''$?
\item Как связаны между собой $\hVar\left(\hb_2\right)$, $\hVar\left(\hb_2'\right)$ и $\hVar\left(\hb_2''\right)$?
\item Как выглядит матрица $\hVar\left(\hb'\right)$?
\item Как связаны между собой $t$-статистики $t_{\hb_2}$, $t_{\hb_2'}$ и $t_{\hb_2''}$?
\item Как связаны между собой $R^2$, $R^{2\prime}$ и $R^{2\prime\prime}$?
\item В нескольких предложениях прокомментируйте последствия перехода к стандартизированным переменным
\end{enumerate}



\item Регрессионная модель  задана в матричном виде при помощи уравнения $y=X\beta+\varepsilon$, где $\beta=(\beta_1,\beta_2,\beta_3)'$.
Известно, что $\E(\varepsilon)=0$  и  $\Var(\varepsilon)=\sigma^2\cdot I$.
Известно также, что 

$y=\left(
\begin{array}{c} 
1\\ 
2\\ 
3\\ 
4\\ 
5
\end{array}\right)$, 
$X=\left(\begin{array}{ccc}
1 & 0 & 0 \\
1 & 0 & 0 \\
1 & 0 & 0 \\
1 & 1 & 0 \\
1 & 1 & 1 
\end{array}\right)$.


Для удобства расчетов приведены матрицы 


$X'X=\left(
\begin{array}{ccc} 
5 & 2 & 1\\ 
2 & 2 & 1\\ 
1 & 1 & 1 
\end{array}\right)$ и $(X'X)^{-1}=\frac{1}{3}\left(
\begin{array}{ccc} 
1 & -1 & 0 \\
-1 & 4 & -3 \\
0 & -3 & 6
\end{array}\right)$.

\begin{enumerate}
\item Укажите число наблюдений.
\item Укажите число регрессоров с учетом свободного члена.
\item Запишите модель в скалярном виде
\item Рассчитайте $TSS=\sum (y_i-\bar{y})^2$, $RSS=\sum (y_i-\hat{y}_i)^2$ и $ESS=\sum (\hat{y}_i-\bar{y})^2$.
\item Рассчитайте при помощи метода наименьших квадратов $\hb$, оценку для вектора неизвестных коэффициентов.
\item Чему равен $\he_5$, МНК-остаток регрессии, соответствующий 5-ому наблюдению?
\item Чему равен $R^2$  в модели? Прокомментируйте полученное значение с точки зрения качества оцененного уравнения регрессии.
\item Используя приведенные выше данные, рассчитайте несмещенную оценку для неизвестного параметра $\sigma^2$ регрессионной модели.
\item Рассчитайте $\widehat{\Var}(\hb)$, оценку для ковариационной матрицы вектора МНК-коэффициентов $\hb$.  
\item Найдите $\widehat{\Var}(\hb_1)$, несмещенную оценку дисперсии МНК-коэффициента $\hb_1$.
\item Найдите $\widehat{\Var}(\hb_2)$, несмещенную оценку дисперсии МНК-коэффициента $\hb_2$.
\item Найдите $\widehat{\Cov}(\hb_1,\hb_2)$, несмещенную оценку ковариации МНК-коэффициентов $\hb_1$ и $\hb_2$.
\item Найдите $\widehat{\Var}(\hb_1+\hb_2)$, $\widehat{\Var}(\hb_1-\hb_2)$, $\widehat{\Var}(\hb_1+\hb_2+\hb_3)$, $\widehat{\Var}(\hb_1+\hb_2-2\hb_3)$
\item Найдите $\hCorr(\hb_1,\hb_2)$, оценку коэффициента корреляции МНК-коэффициентов $\hb_1$ и $\hb_2$.
\item Найдите $s_{\hb_1}$, стандартную ошибку МНК-коэффициента $\hb_1$.
\item Рассчитайте выборочную ковариацию $y$ и $\hy$.
\item Найдите выборочную дисперсию $y$, выборочную дисперсию $\hy$.
\end{enumerate}

\item Теорема Фриша-Вау. Регрессоры разбиты на две группы: матрицу $X_1$ размера $n\times k_1$ и матрицу $X_2$ размера $n\times k_2$. Рассмотрим две процедуры:
\begin{enumerate}
\item[M1.] Строим регрессия вектора $y$ на все регрессоры, т.е. оцениваем модель:
\[ 
y=X\beta + \e = X_1 \beta_1 + X_2 \beta_2 +\e 
\]
\item[M2.] Процедура из двух шагов:
\begin{enumerate}
\item Строим регрессию вектора $y$ на все регрессоры первой группы и получаем вектор остатков $M_1 y$, где $M_1=I-X_1(X_1'X_1)^{-1}X_1'$. Строим регрессию каждого регрессора из второй группы на все регрессоры первой группы и получаем в каждом случае вектор остатков. Эти остатки можно записать матрицей $M_1 X_2$. 
\item Строим регрессию вектора $M_1 y$ на остатки $M_1 X_2$.
\end{enumerate}
Другими словами мы оцениваем модель:
\[
M_1 y = M_1 X_2 \gamma_2 + u
\]

\end{enumerate}
\begin{enumerate}
\item Верно ли, что МНК оценки коэффициентов $\hb_2$ и $\hat{\gamma}_2$ совпадают?
\item Верно ли, что остатки в обеих регрессиях совпадают?
\end{enumerate}
\solution{да, да}


\item Всего имеется $100$ наблюдений. Для первых 50-ти наблюдений $X'X=\begin{pmatrix}
50 & 300 \\
300 & 2100
\end{pmatrix}$, $X'y=\begin{pmatrix}
300 & 2000
\end{pmatrix}'$, $y'y=2100$. По последним 50-ти наблюдениям: $X'X=\begin{pmatrix}
50 & 300 \\
300 & 2100
\end{pmatrix}$, $X'y=\begin{pmatrix}
300 & 2200
\end{pmatrix}'$, $y'y=2500$. По первым 50-ти наблюдениям оценивается модель $y_i = \beta_1 + \beta_2 x_i + \e_i$, по последним 50-ти наблюдениям оценивается модель $y_i = \gamma_1 + \gamma_2 x_i + \e_i$. Предположеним, что во всех 100 наблюдениях $\e_i$ независимы и нормальны $N(0;\sigma^2)$. На уровне значимости 5\% проверьте гипотезу $H_0: \, \beta=\gamma$.




\end{enumerate}