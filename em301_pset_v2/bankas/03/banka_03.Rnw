\documentclass[pdftex,12pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage[russian]{babel}

\usepackage[paper=a4paper,top=13.5mm, bottom=13.5mm,left=16.5mm,right=13.5mm,includefoot]{geometry}
\usepackage[pdftex,unicode,colorlinks=true,urlcolor=blue,hyperindex,breaklinks]{hyperref} 

\usepackage{dcolumn}
\usepackage{color}
%\usepackage{wasysym}
\usepackage{floatflt}
\usepackage{graphicx}
\usepackage{fancyhdr}
\usepackage{booktabs}
\usepackage{longtable}
\usepackage{setspace}
\usepackage{indentfirst} % indent first line

%\usepackage{savesym} % avoid conflicting definitions with amsmath
%\savesymbol{iint}
%\savesymbol{iiint}

\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\def\baselinestretch{1.5}

\DeclareMathOperator{\Cov}{Cov}
\DeclareMathOperator{\Var}{Var}
\DeclareMathOperator{\tr}{tr}
\DeclareMathOperator{\rk}{rk}
\newcommand{\E}{\mathbb{E}}
%\newcommand{\P}{\mathbb{P}} % somewhere \P is already defined :)
\newcommand{\e}{\varepsilon}

\begin{document}

<<message=FALSE, echo=FALSE>>=
require(xtable)
xmatrix <- function(a, environment="pmatrix",output=TRUE) {
  # environment: pmatrix of bmatrix
  
  # override default alignment for xtable
  xa <- xtable(a,align=rep("",ncol(a)+1))
  
  res <- print(xa, 
              floating=FALSE, 
              tabular.environment=environment, 
              hline.after=NULL, 
              include.rownames=FALSE, 
              include.colnames=FALSE,
              file="junk.txt")
  
  res <- paste("\\ensuremath{",res,"}",sep="")

  if (output) cat(res)
  return(invisible(res))
}
@

\begin{enumerate}

\item Пусть $\e = (\e_1, \e_2, \e_3)^\prime \sim N (0,I)$ и матрица $A$ представлена ниже. Найдите $\E(\e^\prime A\e)$ и распределение случайной величины $\e^\prime A\e$.

\begin{enumerate}
\item 
$\begin{pmatrix} 2/3 & -1/3 & 1/3 \\ -1/3 & 2/3 & 1/3 \\ 1/3 & 1/3 & 2/3 \end{pmatrix}$
<<results='asis', echo=FALSE>>=
a <- matrix(c(2/3, -1/3, 1/3, -1/3, 2/3, 1/3, 1/3, 1/3, 2/3), nrow=3, ncol=3, byrow = FALSE, dimnames = NULL)
@
\item 
$\begin{pmatrix} 2/3 & -1/3 & -1/3 \\ -1/3 & 2/3 & -1/3 \\ -1/3 & -1/3 & 2/3 \end{pmatrix}$
<<results='asis', echo=FALSE>>=
e <- matrix(c(2/3, -1/3, -1/3, -1/3, 2/3, -1/3, -1/3, -1/3, 2/3), nrow=3, ncol=3, byrow = FALSE, dimnames = NULL)
@
\item 
$\begin{pmatrix} 1/3 & 1/3 & -1/3 \\ 1/3 & 1/3 & -1/3 \\ -1/3 & -1/3 & 1/3 \end{pmatrix}$
<<results='asis', echo=FALSE>>=
b <- matrix(c(1/3, 1/3, -1/3, 1/3, 1/3, -1/3, -1/3, -1/3, 1/3), nrow=3, ncol=3, byrow = FALSE, dimnames = NULL)
@
\item
$\begin{pmatrix} 1/3 & 1/3 & 1/3 \\ 1/3 & 1/3 & 1/3 \\ 1/3 & 1/3 & 1/3 \end{pmatrix}$
<<results='asis', echo=FALSE>>=
f <- matrix(c(1/3, 1/3, 1/3, 1/3, 1/3, 1/3, 1/3, 1/3, 1/3), nrow=3, ncol=3, byrow = FALSE, dimnames = NULL)
@
\item
$\begin{pmatrix} 1/2 & 0 & 1/2 \\ 0 & 1 & 0 \\ 1/2 & 0 & 1/2 \end{pmatrix}$
<<results='asis', echo=FALSE>>=
g <- matrix(c(1/2, 0, 1/2, 0, 1, 0, 1/2, 0, 1/2), nrow=3, ncol=3, byrow = FALSE, dimnames = NULL)
@
\item
$\begin{pmatrix} 1/2 & 0 & -1/2 \\ 0 & 1 & 0 \\ -1/2 & 0 & 1/2 \end{pmatrix}$
<<results='asis', echo=FALSE>>=
h <- matrix(c(1/2, 0, -1/2, 0, 0, 0, -1/2, 0, 1/2), nrow=3, ncol=3, byrow = FALSE, dimnames = NULL)
@
\item
$\begin{pmatrix} 1/2 & -1/2 & 0 \\ -1/2 & 1/2 & 0 \\ 0 & 0 & 1 \end{pmatrix}$
<<results='asis', echo=FALSE>>=
i <- matrix(c(1/2, -1/2, 0, -1/2, 1/2, 0, 0, 0, 1), nrow=3, ncol=3, byrow = FALSE, dimnames = NULL)
@
\item
$\begin{pmatrix} 1/2 & 1/2 & 0 \\ 1/2 & 1/2 & 0 \\ 0 & 0 & 0 \end{pmatrix}$
<<results='asis', echo=FALSE>>=
j <- matrix(c(1/2, 1/2, 0, 1/2, 1/2, 0, 0, 0, 0), nrow=3, ncol=3, byrow = FALSE, dimnames = NULL)
@
\item 
$\begin{pmatrix} 0.8 & 0.4 & 0 \\ 0.4 & 0.2 & 0 \\ 0 & 0 & 1 \end{pmatrix}$
<<results='asis', echo=FALSE>>=
c <- matrix(c(0.8, 0.4, 0, 0.4, 0.2, 0, 0, 0, 1), nrow=3, ncol=3, byrow = FALSE, dimnames = NULL)
@
\item 
$\begin{pmatrix} 0.2 & -0.4 & 0 \\ -0.4 & 0.8 & 0 \\ 0 & 0 & 0 \end{pmatrix}$
<<results='asis', echo=FALSE>>=
d <- matrix(c(0.2, -0.4, 0, -0.4, 0.8, 0, 0, 0, 0), nrow=3, ncol=3, byrow = FALSE, dimnames = NULL)
@
\end{enumerate}


\item Пусть $\e = (\e_1, \e_2, \e_3)^\prime \sim N (0,I)$. Найдите $\E(\e^\prime P\e)$ и распределение случайной величины $\e^\prime P\e$, если $P = X(X^\prime X)^{-1}X^\prime$ и матрица $X^\prime$ представлена ниже.
\begin{enumerate}
\item
<<results='asis',echo=FALSE>>=
A <- matrix(as.integer(c(1,1,1)), nrow=1, ncol=3, byrow = FALSE, dimnames = NULL)
xmatrix(A)
@
\item
<<results='asis',echo=FALSE>>=
B <- matrix(as.integer(c(1,2,3)), nrow=1, ncol=3, byrow = FALSE, dimnames = NULL)
xmatrix(B)
@
\item
<<results='asis',echo=FALSE>>=
C <- matrix(as.integer(c(1,0,1,0,1,1)), nrow=2, ncol=3, byrow = FALSE, dimnames = NULL)
xmatrix(C)
@
\item
<<results='asis',echo=FALSE>>=
D <- matrix(as.integer(c(1,1,1,2,1,3)), nrow=2, ncol=3, byrow = FALSE, dimnames = NULL)
xmatrix(D)
@
\item
<<results='asis',echo=FALSE>>=
E <- matrix(as.integer(c(1,0,0,1,1,0,1,1,1)), nrow=3, ncol=3, byrow = FALSE, dimnames = NULL)
xmatrix(E)
@
\end{enumerate}

\item Пусть $y = X\beta + \e$ --- регрессионная модель.
\begin{enumerate}
\item Сформулируйте теорему Гаусса-Маркова
\item Верно ли, что оценка $\hat{\beta} = (X^\prime X)^{-1}X^\prime Y$ несмещённая?
\item В условиях теоремы Гаусса-Маркова найдите ковариационную матрицу $\hat{\beta}$
\end{enumerate}

\item Пусть $y = X\beta + \e$ --- регрессионная модель и $\tilde{\beta} = ((X^\prime X)^{-1}X^\prime + A)Y$ --- несмещённая оценка вектора неизвестных параметров $\beta$. Верно ли, что $AX=0$?

\item Пусть $y = X\beta + \e$ --- регрессионная модель, $X = \begin{pmatrix} 1 & 0 \\ 1 & 1 \\ 1 & 1 \end{pmatrix}$, $y = \begin{pmatrix} 1 \\ 2 \\ 3 \end{pmatrix}$, $\beta = \begin{pmatrix} \beta_1 \\ \beta_2 \end{pmatrix}$, $\e = \begin{pmatrix} \e_1 \\ \e_2 \\ \e_3 \end{pmatrix}$, $\E(\e)$ = 0, $Var(\e) = \sigma^2 I$. Найдите коэффициент корреляции $corr(\hat{\beta_1},\hat{\beta_2})$.

<<echo=FALSE,results='asis'>>=
X <- matrix(as.integer(c(1,1,1,0,1,1)), nrow=3, ncol=2, byrow = FALSE, dimnames = NULL)
Y <- matrix(as.integer(c(1,2,3)), nrow=3, ncol=1, byrow = FALSE, dimnames = NULL)
@


\item Пусть  $X = \begin{pmatrix} X_1 \\ X_2 \end{pmatrix}$, $\E(X) = \begin{pmatrix} 1 \\ 2 \end{pmatrix}$, $Var(X) = \begin{pmatrix} 2 & 1 \\ 1 & 2 \end{pmatrix}$. Найдите $\E(Y)$, $Var(Y)$ и $\E(Z)$, если
\begin{enumerate}
\item $Y = X - \E(X)$
\item $Y = Var(X)X$
\item $Y = Var(X)(X - \E(X))$
\item $Y = Var(X)^{-1}(X - \E(X))$
\item $Y = Var(X)^{-1/2}(X - \E(X))$
\item $Z = (X - \E(X))^\prime Var(X)(X - \E(X))$
\item $Z = (X - \E(X))^\prime Var(X)^{-1}(X - \E(X))$
\item $Z = X^\prime Var(X)X$
\item $Z = X^\prime Var(X)^{-1}X$
\end{enumerate}

<<echo=FALSE,results='asis'>>=
ex <- matrix(as.integer(c(1,2)), nrow=2, ncol=1, byrow = FALSE, dimnames = NULL)
varx <- matrix(as.integer(c(2,1,1,2)), nrow=2, ncol=2, byrow = FALSE, dimnames = NULL)
@

\item Пусть  $X = \begin{pmatrix} X_1 \\ X_2 \end{pmatrix}$, $\E(X) = \begin{pmatrix} 1 \\ 4 \end{pmatrix}$, $Var(X) = \begin{pmatrix} 4 & 1 \\ 1 & 4 \end{pmatrix}$. Найдите $\E(Y)$, $Var(Y)$ и $\E(Z)$, если
\begin{enumerate}
\item $Y = X - \E(X)$
\item $Y = Var(X)X$
\item $Y = Var(X)(X - \E(X))$
\item $Y = Var(X)^{-1}(X - \E(X))$
\item $Y = Var(X)^{-1/2}(X - \E(X))$
\item $Z = (X - \E(X))^\prime Var(X)(X - \E(X))$
\item $Z = (X - \E(X))^\prime Var(X)^{-1}(X - \E(X))$
\item $Z = X^\prime Var(X)X$
\item $Z = X^\prime Var(X)^{-1}X$
\end{enumerate}

<<echo=FALSE,results='asis'>>=
ex1 <- matrix(as.integer(c(1,4)), nrow=2, ncol=1, byrow = FALSE, dimnames = NULL)
varx1 <- matrix(as.integer(c(4,1,1,4)), nrow=2, ncol=2, byrow = FALSE, dimnames = NULL)
@

\item Пусть $y = X\beta + \e$ --- регрессионная модель, где $\beta = \begin{pmatrix} \beta_1 \\ \beta_2 \\ \beta_3 \end{pmatrix}$. Пусть $Z = XD$, где $D = \begin{pmatrix} 1 & 1 & 0 \\ 0 & 1 & 1 \\ 0 & 0 & 1 \end{pmatrix}$. Рассмотрите <<новую>> регрессионную модель $y = Z\alpha + u$, где $\alpha = \begin{pmatrix} \alpha_1 \\ \alpha_2 \\ \alpha_3 \end{pmatrix}$. Определите, как выражаются <<новые>> МНК-коэффициенты через <<старые>>.

<<echo=FALSE,results='asis'>>=
alpha <- matrix(as.integer(c(1,0,0,1,1,0,0,1,1)), nrow=3, ncol=3, byrow = FALSE, dimnames = NULL)
@

\item Пусть $y = X\beta + \e$ --- регрессионная модель, где $\beta = \begin{pmatrix} \beta_1 \\ \beta_2 \\ \beta_3 \end{pmatrix}$. Пусть $Z = XD$, где $D = \begin{pmatrix} 1 & 1 & 1 \\ 0 & 1 & 1 \\ 0 & 0 & 1 \end{pmatrix}$. Рассмотрите <<новую>> регрессионную модель $y = Z\alpha + u$, где $\alpha = \begin{pmatrix} \alpha_1 \\ \alpha_2 \\ \alpha_3 \end{pmatrix}$. Определите, как выражаются <<новые>> МНК-коэффициенты через <<старые>>.

<<echo=FALSE,results='asis'>>=
alpha2 <- matrix(as.integer(c(1,0,0,1,1,0,1,1,1)), nrow=3, ncol=3, byrow = FALSE, dimnames = NULL)
@

\item Пусть $y = X\beta + \e$ --- регрессионная модель, где $\beta = \begin{pmatrix} \beta_1 \\ \beta_2 \\ \beta_3 \end{pmatrix}$. Пусть $Z = XD$, где $D = \begin{pmatrix} 1 & 0 & 0 \\ 0 & 1 & 1 \\ 0 & 0 & 1 \end{pmatrix}$. Рассмотрите <<новую>> регрессионную модель $y = Z\alpha + u$, где $\alpha = \begin{pmatrix} \alpha_1 \\ \alpha_2 \\ \alpha_3 \end{pmatrix}$. Определите, как выражаются <<новые>> МНК-коэффициенты через <<старые>>.

<<echo=FALSE,results='asis'>>=
alpha3 <- matrix(as.integer(c(1,0,0,0,1,0,0,1,1)), nrow=3, ncol=3, byrow = FALSE, dimnames = NULL)
@

\item Пусть $\e = \begin{pmatrix} \e_1 \\ \e_2 \\ \e_3 \end{pmatrix} \sim N (0,\sigma^2 I)$, $i = (1,\dots,1)^\prime$ --- вектор из n единиц, $\pi=i(i^\prime i)^{-1}i^\prime$,
$X$ --- матрица размера ${n \times k}$, $P = X(X^\prime X)^{-1}X^\prime$. Найдите 
\begin{enumerate}
\item $\E(\e^\prime (P - \pi)\e)$ 
\item $\E(\e^\prime (I - \pi)\e)$
\item $\E(\e^\prime P \e)$
\item $\E(\sum_{i=1}^n {(\e_i - \bar{\e})}^2)$
\end{enumerate}

\item Пусть $y = X\beta + \e$ --- регрессионная модель. Верно ли, что $\hat{\e}^\prime \hat{y}=0$ и $\hat{y}^\prime \hat{\e}=0$?

\item Пусть $\e = (\e_1, \e_2, \e_3)^\prime \sim N (0,4I)$, $A = \begin{pmatrix} 4 & 1 & 1 \\ 1 & 3 & 1 \\ 1 & 1 & 2 \end{pmatrix}$. Найдите

<<echo=FALSE,results='asis'>>=
m_a <- matrix(as.integer(c(4,1,1,1,3,1,1,1,2)), nrow=3, ncol=3, byrow = FALSE, dimnames = NULL)
@

\begin{enumerate}
\item $\E(\e^\prime A\e)$
\item $\E(\e^\prime (I - A)\e)$ 
\end{enumerate}

\item Для нормальной регрессии с 5-ю факторами (включая свободный член) известны границы симметричного по вероятности 80$\%$ доверительного интервала для дисперсии $\sigma_{\e}^2$: $A$ = 45, $B$ = 87.942.

\begin{enumerate}
\item Определите количество наблюдений в выборке
\item Вычислите $\hat{\sigma}_{\e}^2$
\end{enumerate}

Решение

\begin{enumerate}
\item Поскольку $\frac{\hat{\sigma}_{\e}^2(n-k)}{\sigma_{\e}^2} \sim \chi ^2(n-k)$, где $\hat{\sigma}_{\e}^2 = \frac{RSS}{n-k}$, $k$ = 5. $P(\chi_{l}^2 < \frac{\hat{\sigma}_{\e}^2}{\sigma_{\e}^2} < \chi_{u}^2) = 0.8$. Преобразовав, получим $P(\frac{\hat{\sigma}_{\e}^2(n-5)}{\chi_{u}^2} < \sigma_{\e}^2 < \frac{\hat{\sigma}_{\e}^2(n-5)}{\chi_{l}^2}) = 0.8$, где $\chi_{u}^2 = \chi_{n-5; 0.1} ^2$, $\chi_{l}^2 = \chi_{n-5; 0.9} ^2$ --- соответствующие квантили. По условию $\frac{\hat{\sigma}_{\e}^2(n-5)}{\chi_{l}^2} = A = 45, \frac{\hat{\sigma}_{\e}^2(n-5)}{\chi_{u}^2} = B = 87.942.$ Поделим $B$ на $A$, отсюда следует $\frac{\chi_{u}^2}{\chi_{l}^2} = 1.95426.$ Перебором квантилей в таблице для хи-квадрат распределения мы находим, что $\frac{\chi_{30; 0.1}^2}{\chi_{30; 0.9}^2} = \frac{40.256}{20.599} = 1.95426.$ Значит, $n - 5 = 30$, отсюда следует, что $n = 35.$
\item $\hat{\sigma}_{\e}^2 = 45 \frac{\chi_{u}^2}{n-5} = 45 \frac{40.256}{30} = 60.384$ 
\end{enumerate}


\item Пусть $y = X\beta + \e$ --- регрессионная модель, где $\E(\e)=0$, $\Var(\e)=\sigma_{\e}^2 I$. Пусть $A$ --- матрица размера $k \times k, det(A) \not= 0, A = const.$ Совершается преобразование регрессоров по правилу $Z=XA$. В преобразованных регрессорах уравнение выглядит так: $y = Z\gamma + u$, где $\E(u)=0$, $\Var(u)=\sigma_{u}^2 I.$

\begin{enumerate}
\item Как связаны между собой МНК-оценки $\hat{\beta}$ и $\hat{\gamma}$?
\item Как связаны между собой векторы остатков регрессий?
\item Как связаны между собой прогнозные значения, полученные по двум регрессиям?
\end{enumerate}

Решение

\begin{enumerate}
\item $\hat{\gamma} = (Z^\prime Z)^{-1}Z^\prime y = A^{-1}(X^\prime X)^{-1}(A^\prime)^{-1}A^\prime X^\prime y = A^{-1}(X^\prime X)^{-1} X^\prime y = A^{-1}\hat{\beta}$
\item $\hat{u} = y - Z\hat{\gamma} = y - XAA^{-1}\hat{\beta} = y - X\hat{\beta} = \hat{\e}$
\item Пусть $z^0 = \begin{pmatrix} 1 & z_1^0 & \dots & z_{k-1}^0 \end{pmatrix}$ --- вектор размера $1 \times k$ и $x^0 = \begin{pmatrix} 1 & x_1^0 & \dots & x_{k-1}^0 \end{pmatrix}$ --- вектор размера $1 \times k$. Оба эти вектора представляют собой значения факторов. Тогда $z^0 = x^0 A$ и прогнозное значение для регрессии с преобразованными факторами равно $z^0 \hat{\gamma} = x^0 AA^{-1} \hat{\beta} = x^0 \hat{\beta}$ прогнозному значению для регрессии с исходными факторами. 
\end{enumerate}

\item Рассмотрим оценку вида $\tilde{\beta} = ((X^\prime X)^{-1} + \gamma I)X^\prime y$ для вектора коэффициентов регрессионного уравнения $y = X\beta + \e$, удовлетворяющего условиям классической регрессионной модели. Найдите $\E(\tilde{\beta})$ и $Var(\tilde{\beta}).$ 

Решение

\begin{enumerate}
\item $\E(\tilde{\beta}) = ((X^\prime X)^{-1} + \gamma I)X^\prime \E(y) = ((X^\prime X)^{-1} + \gamma I)X^\prime X\beta = \beta + \gamma X^\prime X\beta$
\item $Var(\tilde{\beta}) = Var(((X^\prime X)^{-1} + \gamma I)X^\prime y) = Var(((X^\prime X)^{-1} + \gamma I)X^\prime \e) = $

$ = (((X^\prime X)^{-1} + \gamma I)X^\prime)Var(\e)(((X^\prime X)^{-1} + \gamma I)X^\prime)^\prime = $

$ = (((X^\prime X)^{-1} + \gamma I)X^\prime)\sigma_{\e}^2 I(((X^\prime X)^{-1} + \gamma I)X^\prime)^\prime = 
\sigma_{\e}^2((X^\prime X)^{-1} + \gamma I)X^\prime X((X^\prime X)^{-1} + \gamma I) = $

$ = \sigma_{\e}^2((X^\prime X)^{-1} + \gamma I)(I + \gamma X^\prime X) = \sigma_{\e}^2((X^\prime X)^{-1} + 2\gamma I + \gamma ^2 X^\prime X)$
\end{enumerate}


\item Верно ли, что при невырожденном преобразовании факторов $R^2$ не меняется? А именно, пусть заданы две регрессионные модели: $y = X\beta + \e$ и $y = Z\alpha + u$, где $y$ --- вектор размера $n \times 1$, $X$ и $Z$ --- матрицы размера $n \times k$, $\beta$ и $\alpha$ --- вектора рамзера $k \times 1$, $\e$ и $u$ --- вектора размера $n \times 1$, а также $Z=XD$, $det(D) \not= 0.$ Верно ли, что коэффициенты детерминации представленных выше моделей равны между собой?

\item Верно ли, что при невырожденном преобразовании факторов $RSS$ не меняется. А именно, пусть заданы две регрессиионные модели: $y = X\beta + \e$ и $y = Z\alpha + u$, где $y$ --- вектор размера $n \times 1$, $X$ и $Z$ --- матрицы размера $n \times k$, $\beta$ и $\alpha$ --- вектора рамзера $k \times 1$, $\e$ и $u$ --- вектора размера $n \times 1$, а также $Z=XD$, $det(D) \not= 0.$ Верно ли, что сумма квадратов остатков в представленных выше моделях равны между собой?

\item Пусть $y_i = \beta_1 + \beta_2 X_i + \e_i$ и $i = 1, \dots, 5$ --- классическая регрессионная модель. Также имеются следующие данные: $\sum_{i=1}^5 y_i^2 = 55, \sum_{i=1}^5 X_i^2 = 3, \sum_{i=1}^5 X_iy_i = 12, \sum_{i=1}^5 y_i = 15, \sum_{i=1}^5 X_i = 3.$ Используя их, найдите:

\begin{enumerate}
\item $\hat{\beta_1}$ и $\hat{\beta_2}$
\item $corr(\hat{\beta_1}, \hat{\beta_2})$
\item $TSS$
\item $ESS$
\item $RSS$
\item $R^2$
\item $\hat{\sigma}^2$
\end{enumerate}

Проверьте следующие гипотезы:
\begin{enumerate}
\item $\begin{cases}  H_0: \beta_2 = 2  \\ H_1: \beta_2 \not= 2 \end{cases}$
\item $\begin{cases}  H_0: \beta_1 + \beta_2 = 1  \\ H_1: \beta_1 + \beta_2 \not= 1 \end{cases}$
\end{enumerate}

\item Пусть $y_i = \beta_1 + \beta_2 X_i + \e_i$ и $i = 1, \dots, 5$ --- классическая регрессионная модель. Также имеются следующие данные: $\sum_{i=1}^5 y_i^2 = 55, \sum_{i=1}^5 X_i^2 = 2, \sum_{i=1}^5 X_iy_i = 9, \sum_{i=1}^5 y_i = 15, \sum_{i=1}^5 X_i = 2.$ Используя их, найдите:

\begin{enumerate}
\item $\hat{\beta_1}$ и $\hat{\beta_2}$
\item $corr(\hat{\beta_1}, \hat{\beta_2})$
\item $TSS$
\item $ESS$
\item $RSS$
\item $R^2$
\item $\hat{\sigma}^2$
\end{enumerate}

Проверьте следующие гипотезы:
\begin{enumerate}
\item $\begin{cases}  H_0: \beta_2 = 2  \\ H_1: \beta_2 \not= 2 \end{cases}$
\item $\begin{cases}  H_0: \beta_1 + \beta_2 = 1  \\ H_1: \beta_1 + \beta_2 \not= 1 \end{cases}$
\end{enumerate}







\end{enumerate}

\end{document}